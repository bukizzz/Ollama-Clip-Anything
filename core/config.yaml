# core/config.yaml
# Unified configuration file for Ollama-Clip-Anything

# Directory settings
temp_dir: "temp_processing"
state_file: ".temp_clips.json"
output_dir: "videos"
clip_prefix: "clip"
b_roll_assets_dir: "train2014"
log_dir: "logs"

# Model settings
whisper_model: "base"
llm_model: "gemini-2.5-flash" # Default LLM for general tasks

# LLM configuration (for specific LLM interactions)
llm:
  min_model: "MiniCPM"
  image_model: "llava:latest" # Model for image recognition tasks
  api_keys:
    openai: "YOUR_OPENAI_KEY"
    ollama: "http://localhost:11434"
    gemini: "AIzaSyC1PN8c7Vn-O3q-ZP1b8RjaOx6aNP--d5Y" # Added for Gemini API key
  ollama_keep_alive: 2 # Keep Ollama models loaded indefinitely
  # Removed redundant gemini_model entry

# Subtitle settings
subtitle_font_families: ['Impact', 'Arial Black', 'Bebas Neue']
subtitle_background_color: 'FFA500'
subtitle_border_radius: 10
subtitle_font_size: 28
subtitle_font_color: 'FFFFFF'
subtitle_outline_color: '000000'
subtitle_shadow_color: '000000'
highlight_font_color: '00FFFF'
highlight_outline_color: '000000'

# Video processing settings
clip_duration_min: 30 # Minimum desired clip duration in seconds
clip_duration_max: 120 # Maximum desired clip duration in seconds
clip_validation_min: 30 # Minimum valid clip duration for internal validation
clip_validation_max: 120 # Maximum valid clip duration for internal validation
smoothing_factor: 0.1

# LLM Interaction Settings (general)
llm_max_retries: 5
llm_min_clips_needed: 1 # Changed from 10 to 1

# Personalization settings
custom_clip_themes: [] # Placeholder for custom clip themes

# Agent Configuration (enable/disable agents)
agents:
  video_input_agent: true
  storyboarding_agent: true
  content_alignment_agent: true
  audio_transcription_agent: true
  broll_analysis_agent: true
  llm_selection_agent: true
  video_analysis_agent: true
  video_editing_agent: true
  results_summary_agent: true

# Encoding settings
video_encoder: "h264_nvenc"
ffmpeg_path: "/usr/local/bin/ffmpeg" # Path to FFmpeg executable. Set to None to use system's PATH.
ffmpeg_global_params:
  - '-pix_fmt'
  - 'yuv420p'
  - '-movflags'
  - '+faststart'
ffmpeg_encoder_params:
  h264_nvenc:
    - '-preset'
    - 'p5'
    - '-tune'
    - 'hq'
    - '-rc'
    - 'vbr_hq'
    - '20'
  hevc_nvenc:
    - '-preset'
    - 'p5'
    - '-tune'
    - 'hq'
  av1_nvenc:
    - '-preset'
    - 'p5'
    - '-tune'
    - 'hq'

# New pipeline configuration sections (agent-specific settings)
engagement_analysis:
  facial_expression_threshold: 0.7
  gesture_detection_threshold: 0.6
  energy_level_threshold: 0.5

video_analysis:
  frame_sample_rate: 5
  batch_size: 5

audio_rhythm:
  tempo_detection_sensitivity: 0.8
  beat_matching_sensitivity: 0.7

layout_detection:
  multi_person_detection_threshold: 0.75
  screen_share_identification_threshold: 0.8

subtitle_animation:
  word_by_word_timing_enabled: true
  emphasis_effects_enabled: true
  speaker_color_coding_enabled: true

music_integration:
  mood_detection_enabled: true
  tempo_matching_enabled: true
  beat_synchronization_enabled: true

intro_narration:
  voice_cloning_enabled: true
  tone_matching_enabled: true
  duration_limit_seconds: 5

qwen_vision:
  frame_extraction_rate_fps: 1
  resolution_settings: "720p"
  temporal_encoding_parameters: "default"
  batch_size: 20
  ollama_qwen_vl_model_name: "qwen2.5-coder:7b"

huggingface_tokens:
  pyannote_audio: "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"

audio_analysis:
  speaker_diarization_enabled: true

llm_selection:
  max_retries: 5
  min_clips: 1
  json_processing_model: llm_model # Changed LLM model
  
scene_detection:
  threshold: 0.3
  min_scene_duration: 1.0
  sample_rate: 1
